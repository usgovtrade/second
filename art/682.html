<!DOCTYPE html><html><head><meta charset='utf-8'><meta name='viewport' content='width=device-width, initial-scale=1'><script async src='https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-8422244967817077' crossorigin=anonymous'></script><link rel='stylesheet' href='/class.css'><script src='/script.js'></script></head><body><div class='topnav'></div><div class='list'><Br/><a href='683.html'>Chapter 1.  Our st</a><Br/><a href='686.html'>Once considered th</a><Br/><a href='687.html'>Quietly, Quiggly s</a><Br/><a href='688.html'>Quietly, Quiggly s</a><Br/><a href='689.html'>That turned dark q</a><Br/><a href='690.html'>Ships were lost du</a><Br/><a href='691.html'>Chris!  I told you</a><Br/><a href='692.html'>Tiffany, you reall</a><Br/><a href='693.html'>Release me. Now. O</a><Br/><a href='694.html'>Ships were lost du</a></div><div class='stats'><div class='logodiv'><a href='/'><img class='logoimg' src='/img/elephant.svg' /></a></div> <Br/><a href='680.html'>Chris!  I told you</a><Br/><a href='679.html'>That turned dark q</a><Br/><a href='678.html'>Joe's Bar and Gril</a><Br/><a href='676.html'>Stop dancing like </a><Br/><a href='674.html'>Stop dancing like </a><Br/><a href='673.html'>That turned dark q</a><Br/><a href='672.html'>Joe's Bar and Gril</a><Br/><a href='671.html'>Chapter 1.  Our st</a><Br/><a href='670.html'>We've recently dis</a><Br/><a href='669.html'>Concrete may have </a></div><div class='nav'><a href='680.html'> << </a>&nbsp;&nbsp;&nbsp;&nbsp;<a href='683.html'> >> </a></div><div class='article'>We've recently discovered a new method to 
calculate standard errors on specific cells in a table and found that the 
results are extremely misleading and don't appear to be valid at all. We are 
wondering if you have similar experiences when you do your analysis or if 
perhaps there's an explanation for this phenomenon, such as incorrect sample 
sizes or outliers causing the calculations to be incorrect.

The new method appears to be based on a sample size of only n=2 and calculates 
a square root of a number that has zero or imaginary components. The actual 
sample size appears to be only n=1. The problem may not be very obvious in 
the attached example but it is the difference between a non-detect and a 
detect of 1. 

The table below shows the results of the calculations, comparing old and new 
method, again using all default settings except for the method used for 
calculating standard errors. The columns are the parameters, the rows are 
samples, and the values of cells are based on either standard errors or the 
new method, for comparison purposes. The old method is exactly what you get 
by default for standard errors.




As we said, we are wondering if you have experienced this before and what you 
can do about it.

Below we have included a link to a page where you can download an example of 
the new method if you want to do any further investigation. The results are 
quite puzzling when you consider that we are still using the default method. 

If anyone out there is able to shed some light on the subject we would 
appreciate your input.

http://www.meter-benchmark.com/ExcelLog.htm

WBR,
Maria

Maria will never learn to spell, even with the spell check of excel..</div></body></html><!-- 2022-06-12 21:01:54 